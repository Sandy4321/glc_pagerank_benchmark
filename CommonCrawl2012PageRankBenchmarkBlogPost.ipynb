{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphLab Create v Common Crawl 2012 WebGraph\n",
    "## or, How I Learned to Stop Worrying about 128B edges and Love the PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here at Dato we highly evaluate openness and transparency. Look at the [Dato Gallery](https://dato.com/learn/gallery/ \"Dato Gallery\"), filled with notebooks you can download and execute on your own machine, to see that that GraphLab Create can really do what we say it does. So when it came to benchmarks, we said, why not upload a notebook about it?\n",
    "\n",
    "This notebook will describe our **PageRank benchmark**. The dataset for this benchmark is the web itself - compiled by good people from [commoncrawl.org](http://commoncrawl.org/) in 2012. You can [download the dataset from here](http://webdatacommons.org/hyperlinkgraph/2012-08/download.html#toc0). We will run the **PageRank** algorithm over a network of **3.5 billion nodes** and **128 billion links**. Each node represents a web page, and each link - a hyperlink between two pages.\n",
    "\n",
    "Running this benchmark will prove you how powerful and robust GraphLab is - not many systems with graphs of this size. However, unlike other notebooks in the gallery, we **don't recommend running this notebook on your laptop!**. Instead, this notebook will describe how to run this benchmark on an EC2 instance in the Amazon Web Services (AWS) cloud.\n",
    "\n",
    "We'll be using an **r3.8xlarge** EC2 instance,. That's a strong machine,\n",
    "with 32 cores, 244 Gigabytes of RAM, and 2 SSD drives, each sized 320 GBs.\n",
    "If you can access a physical machine of this calibre, expect similar results.\n",
    "\n",
    "For now, read our guide for **launching and setting-up an EC2 instance** for this benchmark. This guide will describe how to:\n",
    "\n",
    "1. Launch an EC2 instance,\n",
    "2. Install GraphLab Create and Jupyter (formerly *IPython Notebook*) on it,\n",
    "3. Download this benchmark notebook to the EC2 machine,\n",
    "4. Connect to your Jupyter instance running on the EC2 machine and run the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "The [dataset's webpage says](http://webdatacommons.org/hyperlinkgraph/2012-08/download.html#toc0):\n",
    " \n",
    "    Downloading the page graph: The page graph (arc and indes files) are, due to their size split into in small files of around 500 MB. These files can be downloaded using ```wget -i http://webdatacommons.org/hyperlinkgraph/2012-08/data/index.list.txt``` for the index files and respectively ```wget -i http://webdatacommons.org/hyperlinkgraph/2012-08/data/arc.list.txt``` for the arc files.\n",
    "\n",
    "Examining http://webdatacommons.org/hyperlinkgraph/2012-08/data/arc.list.txt , we will find a list of URLs pointing at .gz files.\n",
    "\n",
    "```\n",
    "http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/network/part-r-00000.gz\n",
    "http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/network/part-r-00001.gz\n",
    "...\n",
    "http://data.dws.informatik.uni-mannheim.de/hyperlinkgraph/2012-08/network/part-r-00696.gz\n",
    "```\n",
    "\n",
    "As CommonCrawl's own documentation says, each of these gzip files weights ~500 MBs. Here is the `head` of the first file (`part-r-00000.gz`):\n",
    "\n",
    "```\n",
    "0\t739935047\n",
    "1\t741742773\n",
    "2\t741745070\n",
    "```\n",
    "\n",
    "This is a very common format for storing graph edges - id1-TAB-id2.\n",
    "While GraphLab could handle loading such a graph, we saved you the the trouble of downloading the files to your EC2 instance and uploaded it to an open S3 bucket. To access it you only need AWS credentials.\n",
    "\n",
    "The SGraph created by loading the data is stored in binary form in this bucket; that way, it weights around 218 GBs, less than the gzipped edges.\n",
    "Also, since this data is stored on Amazon's S3, accessing it from your EC2 instance should be faster than downloading the raw data from CommonCrawl's servers.\n",
    "\n",
    "The bucket is located at [s3://dato-datasets-oregon/webgraphs/sgraph/common_crawl_2012_sgraph/](s3://dato-datasets-oregon/webgraphs/sgraph/common_crawl_2012_sgraph/) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Password protect Jupyter\n",
    "$ jupyter notebook --generate-config\n",
    "$ python -c \"from notebook.auth import passwd; password = passwd(); open('/home/ubuntu/.jupyter/jupyter_notebook_config.py', 'a').write('c.NotebookApp.password = u\\'%s\\'' % (password))\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
